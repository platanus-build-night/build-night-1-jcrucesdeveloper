<!doctype html>
<html lang="en-us">
    <head>
        asdlkfjsad
        <title>stream : Real-time Whisper transcription in WebAssembly</title>

        <style>
            #output {
                width: 100%;
                height: 100%;
                margin: 0 auto;
                margin-top: 10px;
                border-left: 0px;
                border-right: 0px;
                padding-left: 0px;
                padding-right: 0px;
                display: block;
                background-color: black;
                color: white;
                font-size: 10px;
                font-family: 'Lucida Console', Monaco, monospace;
                outline: none;
                white-space: pre;
                overflow-wrap: normal;
                overflow-x: scroll;
            }
        </style>
        <script src="../coi-serviceworker.js"></script>
        <link rel="icon" href="data:,">
    </head>
    <body>
        <div id="main-container">
            <b>stream : Real-time Whisper transcription in WebAssembly</b>

            <br><br>

            You can find more about this project on <a href="https://github.com/ggerganov/whisper.cpp/tree/master/examples/stream.wasm">GitHub</a>.

            <br><br>

            <b>More examples:</b>
                <a href="../">main</a> |
                <a href="../bench.wasm/">bench</a> |
                <a href="../stream.wasm">stream</a> |
                <a href="../command.wasm/">command</a> |

            <br><br>

            <hr>

            Select the model you would like to use, click the "Start" button and start speaking

            <br><br>

            <div id="model-whisper">
                Whisper model: <span id="model-whisper-status"></span>
                <span id="fetch-whisper-progress"></span>
            </div>

            <br>

            <div id="input">
                <button id="start"  onclick="onStart()" disabled>Start</button>
                <button id="stop"   onclick="onStop()" disabled>Stop</button>
                <button id="clear"  onclick="clearCache()">Clear Cache</button>
            </div>

            <br>

            <div id="state">
                Status: <b><span id="state-status">not started</span></b>

                <pre id="state-transcribed">[The transcribed text will be displayed here]</pre>
            </div>

            <hr>

            Debug output:
            <textarea id="output" rows="20"></textarea>

            <br>

            <b>Troubleshooting</b>

            <br><br>

            The page does some heavy computations, so make sure:

            <ul>
                <li>To use a modern web browser (e.g. Chrome, Firefox)</li>
                <li>To use a fast desktop or laptop computer (i.e. not a mobile phone)</li>
                <li>Your browser supports WASM <a href="https://webassembly.org/roadmap/">Fixed-width SIMD</a></li>
            </ul>

            <div class="cell-version">
                <span>
                    |
                    Build time: <span class="nav-link">Fri Apr 25 15:33:11 2025</span> |
                    Commit hash: <a class="nav-link" href="https://github.com/ggerganov/whisper.cpp/commit/50fda73f">50fda73f</a> |
                    Commit subject: <span class="nav-link">ruby : add encoder begin callback related methods (#3076)</span> |
                    <a class="nav-link" href="https://github.com/ggerganov/whisper.cpp/tree/master/examples/stream.wasm">Source Code</a> |
                </span>
            </div>
        </div>

        <script type="text/javascript" src="helpers.js"></script>
        <script type='text/javascript'>
            // web audio context
            var context = null;

            // audio data
            var audio = null;
            var audio0 = null;

            // the stream instance
            var instance = null;

            // model name
            var model_whisper = null;

            var Module = {
                print: printTextarea,
                printErr: printTextarea,
                setStatus: function(text) {
                    printTextarea('js: ' + text);
                },
                monitorRunDependencies: function(left) {
                },
                preRun: function() {
                    printTextarea('js: Preparing ...');
                },
                postRun: function() {
                    printTextarea('js: Initialized successfully!');
                    // Cargar el modelo automáticamente al iniciar
                    loadLocalModel();
                }
            };

            function loadLocalModel() {
                fetch('ggml-base.en.bin')
                    .then(response => response.arrayBuffer())
                    .then(buffer => {
                        const data = new Uint8Array(buffer);
                        storeFS('whisper.bin', data);
                    })
                    .catch(error => {
                        printTextarea('Error loading model: ' + error);
                    });
            }

            function storeFS(fname, buf) {
                try {
                    Module.FS_unlink(fname);
                } catch (e) {
                    // ignore
                }

                Module.FS_createDataFile("/", fname, buf, true, true);

                printTextarea('storeFS: stored model: ' + fname + ' size: ' + buf.length);

                document.getElementById('model-whisper-status').innerHTML = 'loaded!';
                
                // Habilitar el botón Start cuando el modelo esté cargado
                document.getElementById('start').disabled = false;
                document.getElementById('stop').disabled = true;
            }

            //
            // microphone
            //

            const kSampleRate = 16000;
            const kRestartRecording_s = 2;  // Reduced to 2 seconds for faster processing
            const kIntervalAudio_ms = 100;  // Reduced for more frequent updates

            var mediaRecorder = null;
            var doRecording = false;
            var startTime = 0;
            var audioChunks = [];
            var lastAudioTime = 0;
            var stream = null;

            window.AudioContext = window.AudioContext || window.webkitAudioContext;
            window.OfflineAudioContext = window.OfflineAudioContext || window.webkitOfflineAudioContext;

            function stopRecording() {
                Module.set_status("paused");
                doRecording = false;
                audioChunks = [];
                if (context) {
                    context.close();
                    context = null;
                }
                if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                    mediaRecorder.stop();
                }
                if (stream) {
                    stream.getTracks().forEach(function(track) {
                        track.stop();
                    });
                    stream = null;
                }
                printTextarea("js: recording stopped");
                
                document.getElementById('start').disabled = false;
                document.getElementById('stop').disabled = true;
            }

            function processAudio(audioData) {
                if (!instance) {
                    printTextarea("js: no whisper instance available");
                    return;
                }

                const currentTime = Date.now();
                if (currentTime - lastAudioTime < 100) {  // Throttle processing
                    return;
                }
                lastAudioTime = currentTime;

                try {
                    Module.set_audio(instance, audioData);
                    printTextarea("js: audio sent to whisper, size: " + audioData.length);
                    
                    // Force transcription check
                    const transcribed = Module.get_transcribed();
                    if (transcribed) {
                        printTextarea("js: transcription: " + transcribed);
                        document.getElementById('state-transcribed').innerHTML = transcribed;
                    }
                } catch (e) {
                    printTextarea("js: error processing audio: " + e);
                }
            }

            function startRecording() {
                if (!context) {
                    context = new AudioContext({
                        sampleRate: kSampleRate,
                        channelCount: 1,
                        echoCancellation: false,
                        autoGainControl: true,
                        noiseSuppression: true,
                    });
                    printTextarea("js: audio context created");
                }

                Module.set_status("recording");
                printTextarea("js: starting recording");

                document.getElementById('start').disabled = true;
                document.getElementById('stop').disabled = false;

                doRecording = true;
                startTime = Date.now();
                audioChunks = [];

                navigator.mediaDevices.getUserMedia({audio: true, video: false})
                    .then(function(s) {
                        stream = s;
                        mediaRecorder = new MediaRecorder(stream, {
                            mimeType: 'audio/webm',
                            audioBitsPerSecond: 16000
                        });

                        mediaRecorder.ondataavailable = function(e) {
                            if (e.data.size > 0) {
                                audioChunks.push(e.data);
                                printTextarea("js: received audio chunk of size: " + e.data.size);

                                var blob = new Blob(audioChunks, { 'type' : 'audio/webm' });
                                var reader = new FileReader();

                                reader.onload = function(event) {
                                    var buf = new Uint8Array(reader.result);
                                    printTextarea("js: processing audio buffer of size: " + buf.length);

                                    if (!context) {
                                        printTextarea("js: no audio context available");
                                        return;
                                    }

                                    context.decodeAudioData(buf.buffer, function(audioBuffer) {
                                        var offlineContext = new OfflineAudioContext(audioBuffer.numberOfChannels, audioBuffer.length, audioBuffer.sampleRate);
                                        var source = offlineContext.createBufferSource();
                                        source.buffer = audioBuffer;
                                        source.connect(offlineContext.destination);
                                        source.start(0);

                                        offlineContext.startRendering().then(function(renderedBuffer) {
                                            const audioData = renderedBuffer.getChannelData(0);
                                            printTextarea("js: audio processed, size: " + audioData.length);
                                            processAudio(audioData);
                                        });
                                    }, function(e) {
                                        printTextarea("js: error decoding audio: " + e);
                                    });
                                };

                                reader.readAsArrayBuffer(blob);
                            }
                        };

                        mediaRecorder.onstop = function() {
                            if (doRecording) {
                                printTextarea("js: restarting recording");
                                setTimeout(startRecording, 100);
                            }
                        };

                        mediaRecorder.start(kIntervalAudio_ms);
                        printTextarea("js: media recorder started with interval: " + kIntervalAudio_ms);
                    })
                    .catch(function(err) {
                        printTextarea("js: error getting audio stream: " + err);
                    });
            }

            //
            // main
            //

            var nLines = 0;
            var intervalUpdate = null;
            var transcribedAll = '';

            function onStart() {
                if (!instance) {
                    instance = Module.init('whisper.bin');

                    if (instance) {
                        printTextarea("js: whisper initialized, instance: " + instance);
                        
                        // First set the thread count before any other parameters
                        Module.set_n_threads(2);
                        printTextarea("js: set thread count to 2");
                        
                        // Then set other parameters
                        Module.set_params(instance, {
                            language: "en",
                            translate: false,
                            n_threads: 2,
                            offset_ms: 0,
                            duration_ms: 0,
                            max_tokens: 32,
                            audio_ctx: 0,
                            vad_thold: 0.3,  // Lowered for better sensitivity
                            freq_thold: 100.0,  // Lowered for better detection
                            speed_up: false,
                            print_progress: true,
                            print_special: false,
                            print_realtime: true,
                            print_timestamps: false,
                            token_timestamps: false,
                            split_on_word: false,
                            max_len: 0,
                            max_text_tokens: 0,
                            suppress_blank: false,  // Changed to false to see all output
                            suppress_non_speech_tokens: false,  // Changed to false to see all output
                            initial_prompt: "",
                            prompt_tokens: null,
                            prefix: null,
                            temperature: 0.0,
                            compression_ratio_threshold: 2.4,
                            logprob_threshold: -1.0,
                            no_speech_threshold: 0.6
                        });
                        printTextarea("js: whisper parameters set");
                    }
                }

                if (!instance) {
                    printTextarea("js: failed to initialize whisper");
                    return;
                }

                startRecording();

                intervalUpdate = setInterval(function() {
                    var transcribed = Module.get_transcribed();
                    printTextarea("js: checking transcription: " + (transcribed ? "has content" : "null"));

                    if (transcribed != null && transcribed.length > 1) {
                        printTextarea("js: new transcription: " + transcribed);
                        transcribedAll += transcribed + '<br>';
                        nLines++;

                        // Keep fewer lines in memory
                        if (nLines > 3) {
                            var i = transcribedAll.indexOf('<br>');
                            if (i > 0) {
                                transcribedAll = transcribedAll.substring(i + 4);
                                nLines--;
                            }
                        }
                    }

                    document.getElementById('state-status').innerHTML = Module.get_status();
                    document.getElementById('state-transcribed').innerHTML = transcribedAll;
                }, 250);
            }

            function onStop() {
                stopRecording();
            }

        </script>
        <script type="text/javascript" src="../libstream.js"></script>
    </body>
</html>
